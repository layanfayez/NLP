{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "11db9d53-ad05-5351-8a35-e1fed027a0e4",
        "openai_ephemeral_user_id": "cda0f107-3041-5d69-932f-db1b6ea133c2",
        "openai_subdivision1_iso_code": "AE-AJ"
      }
    },
    "noteable": {
      "last_transaction_id": "912c12ee-9de3-4702-a35d-a49af0055c88"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "0d8b2eda-ede3-4609-8ba8-ce5a950cacfa",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "3414b24c-e886-4be9-b43d-1e743bc057fb"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:23:23.197925+00:00",
          "start_time": "2023-06-27T17:23:22.977721+00:00"
        },
        "datalink": {
          "6dbd2647-1f53-48b4-9d3c-0c5e52227a65": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 4,
              "orig_num_rows": 5,
              "orig_size_bytes": 200,
              "truncated_num_cols": 4,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 200,
              "truncated_string_columns": []
            },
            "display_id": "6dbd2647-1f53-48b4-9d3c-0c5e52227a65",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-06-27T17:23:23.039795",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_9a3337feb837494592bcedb71f99926e"
          }
        }
      },
      "execution_count": null,
      "source": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('tweets_dataset.csv')\n\n# Display the first 5 rows of the dataframe\ndf.head()",
      "outputs": []
    },
    {
      "id": "aa08bd14-e14c-4ec5-98eb-58f1328e46ec",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "8fbfa196-4522-48d8-b45c-9d592730e401"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:26:22.450915+00:00",
          "start_time": "2023-06-27T17:26:21.474436+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Split the data into training and testing sets\ntrain_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n\n# Initialize a tokenizer\ntokenizer = Tokenizer()\n\n# Fit the tokenizer on the training data\ntokenizer.fit_on_texts(train_data['Tweet'])\n\n# Tokenize the training and testing data\ntrain_tweets = tokenizer.texts_to_sequences(train_data['Tweet'])\ntest_tweets = tokenizer.texts_to_sequences(test_data['Tweet'])\n\n# Pad the sequences so each sequence is the same length\ntrain_tweets = pad_sequences(train_tweets)\ntest_tweets = pad_sequences(test_tweets, maxlen=train_tweets.shape[1])\n\n# Print the shapes\nprint('Train tweets shape:', train_tweets.shape)\nprint('Test tweets shape:', test_tweets.shape)",
      "outputs": []
    },
    {
      "id": "7fac167a-050c-4ead-aba8-ba6c412846ea",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "374a683a-03d6-4eb3-bbf7-e0c47b103650"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:26:41.196736+00:00",
          "start_time": "2023-06-27T17:26:35.656208+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install keras",
      "outputs": []
    },
    {
      "id": "40c0c549-1aab-44f3-957e-0f5fdfede184",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "a86523bf-d805-4f4e-b733-14613ec6ab2b"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:27:17.771577+00:00",
          "start_time": "2023-06-27T17:27:17.312461+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Split the data into training and testing sets\ntrain_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n\n# Initialize a tokenizer\ntokenizer = Tokenizer()\n\n# Fit the tokenizer on the training data\ntokenizer.fit_on_texts(train_data['Tweet'])\n\n# Tokenize the training and testing data\ntrain_tweets = tokenizer.texts_to_sequences(train_data['Tweet'])\ntest_tweets = tokenizer.texts_to_sequences(test_data['Tweet'])\n\n# Pad the sequences so each sequence is the same length\ntrain_tweets = pad_sequences(train_tweets)\ntest_tweets = pad_sequences(test_tweets, maxlen=train_tweets.shape[1])\n\n# Print the shapes\nprint('Train tweets shape:', train_tweets.shape)\nprint('Test tweets shape:', test_tweets.shape)",
      "outputs": []
    },
    {
      "id": "cee3f93a-ea15-4a6c-ae86-9a7013ea74fd",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4cb7b677-6fc0-442f-ba2e-d5ce3b123956"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:28:20.512741+00:00",
          "start_time": "2023-06-27T17:27:35.073712+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install tensorflow",
      "outputs": []
    },
    {
      "id": "475bc271-bb5c-48d2-8d34-1ff814f67190",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "3d1dc1f7-ff73-4169-80c5-6d6ea576486d"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:28:22.884721+00:00",
          "start_time": "2023-06-27T17:28:20.519565+00:00"
        }
      },
      "execution_count": null,
      "source": "from tensorflow import keras",
      "outputs": []
    },
    {
      "id": "01e66e9d-22b2-4050-b6e9-df69f7a9c1d0",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ce8ecf4b-c677-462f-994a-e02ff5b09bac"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:33:56.914919+00:00",
          "start_time": "2023-06-27T17:33:56.511233+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Split the data into training and testing sets\ntrain_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n\n# Initialize a tokenizer\ntokenizer = Tokenizer()\n\n# Fit the tokenizer on the training data\ntokenizer.fit_on_texts(train_data['Tweet'])\n\n# Tokenize the training and testing data\ntrain_tweets = tokenizer.texts_to_sequences(train_data['Tweet'])\ntest_tweets = tokenizer.texts_to_sequences(test_data['Tweet'])\n\n# Pad the sequences so each sequence is the same length\ntrain_tweets = pad_sequences(train_tweets)\ntest_tweets = pad_sequences(test_tweets, maxlen=train_tweets.shape[1])\n\n# Print the shapes\nprint('Train tweets shape:', train_tweets.shape)\nprint('Test tweets shape:', test_tweets.shape)",
      "outputs": []
    },
    {
      "id": "b88d275c-4c74-4625-85ef-c81759bac7ae",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "3ae286e1-de29-4bf9-990f-dbf39b0b2067"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:34:36.899178+00:00",
          "start_time": "2023-06-27T17:34:36.615787+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Split the data into training and testing sets\ntrain_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n\n# Initialize a tokenizer\ntokenizer = Tokenizer()\n\n# Fit the tokenizer on the training data\ntokenizer.fit_on_texts(train_data['Tweet'])\n\n# Tokenize the training and testing data\ntrain_tweets = tokenizer.texts_to_sequences(train_data['Tweet'])\ntest_tweets = tokenizer.texts_to_sequences(test_data['Tweet'])\n\n# Pad the sequences so each sequence is the same length\ntrain_tweets = pad_sequences(train_tweets)\ntest_tweets = pad_sequences(test_tweets, maxlen=train_tweets.shape[1])\n\n# Print the shapes\nprint('Train tweets shape:', train_tweets.shape)\nprint('Test tweets shape:', test_tweets.shape)",
      "outputs": []
    },
    {
      "id": "defc8d9a-a8c6-4218-9144-478e4807e278",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "92e2e74f-50d6-408e-b838-99f7fa574740"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:36:09.969005+00:00",
          "start_time": "2023-06-27T17:36:09.807387+00:00"
        }
      },
      "execution_count": null,
      "source": "# Create sentiment labels\ntrain_labels = train_data['Tweet'].apply(lambda x: 1 if 'Cristiano Ronaldo' in x or 'Al Nasr' in x else 0)\ntest_labels = test_data['Tweet'].apply(lambda x: 1 if 'Cristiano Ronaldo' in x or 'Al Nasr' in x else 0)\n\n# Print the shapes\nprint('Train labels shape:', train_labels.shape)\nprint('Test labels shape:', test_labels.shape)",
      "outputs": []
    },
    {
      "id": "9363f603-5381-48c9-8b22-643334dc5542",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "d5920277-cc19-4c21-b760-a1896d939439"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:37:42.601569+00:00",
          "start_time": "2023-06-27T17:37:40.654220+00:00"
        }
      },
      "execution_count": null,
      "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, LSTM, Dense\n\n# Define the model\nmodel = Sequential()\n\n# Add an Embedding layer\nmodel.add(Embedding(input_dim=10000, output_dim=32, input_length=train_tweets.shape[1]))\n\n# Add 5 layers of CNN\nfor _ in range(5):\n    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n\n# Add 5 layers of LSTM\nfor _ in range(5):\n    model.add(LSTM(units=32, return_sequences=True))\n\n# Add a Dense layer for classification\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Print the model summary\nmodel.summary()",
      "outputs": []
    },
    {
      "id": "520e248a-f31c-45ee-aae4-991872ed0772",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "3189ab23-c1ac-4d77-9f02-2ba9d822b56e"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:39:15.424266+00:00",
          "start_time": "2023-06-27T17:39:13.476263+00:00"
        }
      },
      "execution_count": null,
      "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, LSTM, Dense\n\n# Define the model\nmodel = Sequential()\n\n# Add an Embedding layer\nmodel.add(Embedding(input_dim=10000, output_dim=32, input_length=train_tweets.shape[1]))\n\n# Add 6 layers of CNN\nfor _ in range(6):\n    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n\n# Add 6 layers of LSTM\nfor _ in range(6):\n    model.add(LSTM(units=32, return_sequences=True))\n\n# Add a Dense layer for classification\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Print the model summary\nmodel.summary()",
      "outputs": []
    },
    {
      "id": "7c02f7ae-32cb-42ed-8a47-8c393b77edb6",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "550c19f9-da0a-46c4-a6ee-759bf2323504"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:48:38.363072+00:00",
          "start_time": "2023-06-27T17:48:35.866012+00:00"
        }
      },
      "execution_count": null,
      "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, LSTM, Dense\n\n# Define the model\nmodel = Sequential()\n\n# Add an Embedding layer\nmodel.add(Embedding(input_dim=10000, output_dim=32, input_length=train_tweets.shape[1]))\n\n# Add 7 layers of CNN\nfor _ in range(7):\n    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n\n# Add 7 layers of LSTM\nfor _ in range(7):\n    model.add(LSTM(units=32, return_sequences=True))\n\n# Add a Dense layer for classification\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Print the model summary\nmodel.summary()",
      "outputs": []
    },
    {
      "id": "4d2e6e79-298b-43a4-87ab-4555f5d37b9e",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "bc486907-182b-455b-b306-7dc221c7e48c"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:54:49.363702+00:00",
          "start_time": "2023-06-27T17:54:49.165261+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n\n# Predict the sentiments for the test data\ny_pred_5 = model.predict_classes(test_tweets)\ny_pred_6 = model_6.predict_classes(test_tweets)\ny_pred_7 = model_7.predict_classes(test_tweets)\n\n# Calculate the confusion matrix\ncm_5 = confusion_matrix(test_sentiments, y_pred_5)\ncm_6 = confusion_matrix(test_sentiments, y_pred_6)\ncm_7 = confusion_matrix(test_sentiments, y_pred_7)\n\n# Calculate the accuracy\naccuracy_5 = accuracy_score(test_sentiments, y_pred_5)\naccuracy_6 = accuracy_score(test_sentiments, y_pred_6)\naccuracy_7 = accuracy_score(test_sentiments, y_pred_7)\n\n# Calculate the precision\nprecision_5 = precision_score(test_sentiments, y_pred_5)\nprecision_6 = precision_score(test_sentiments, y_pred_6)\nprecision_7 = precision_score(test_sentiments, y_pred_7)\n\n# Calculate the F1 score\nf1_5 = f1_score(test_sentiments, y_pred_5)\nf1_6 = f1_score(test_sentiments, y_pred_6)\nf1_7 = f1_score(test_sentiments, y_pred_7)\n\n# Print the results\nprint('5-layer model:')\nprint('Confusion Matrix:', cm_5)\nprint('Accuracy:', accuracy_5)\nprint('Precision:', precision_5)\nprint('F1 Score:', f1_5)\n\nprint('\\n6-layer model:')\nprint('Confusion Matrix:', cm_6)\nprint('Accuracy:', accuracy_6)\nprint('Precision:', precision_6)\nprint('F1 Score:', f1_6)\n\nprint('\\n7-layer model:')\nprint('Confusion Matrix:', cm_7)\nprint('Accuracy:', accuracy_7)\nprint('Precision:', precision_7)\nprint('F1 Score:', f1_7)",
      "outputs": []
    },
    {
      "id": "4fe185a2-ce3a-41ee-82e5-90dd1394adbb",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "6d3911ca-91b8-4ca5-96bf-533c3fab2e42"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:56:01.746537+00:00",
          "start_time": "2023-06-27T17:55:58.113487+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\nimport numpy as np\n\n# Predict the sentiments for the test data\ny_pred_5 = np.round(model.predict(test_tweets)).astype(int)\ny_pred_6 = np.round(model_6.predict(test_tweets)).astype(int)\ny_pred_7 = np.round(model_7.predict(test_tweets)).astype(int)\n\n# Calculate the confusion matrix\ncm_5 = confusion_matrix(test_sentiments, y_pred_5)\ncm_6 = confusion_matrix(test_sentiments, y_pred_6)\ncm_7 = confusion_matrix(test_sentiments, y_pred_7)\n\n# Calculate the accuracy\naccuracy_5 = accuracy_score(test_sentiments, y_pred_5)\naccuracy_6 = accuracy_score(test_sentiments, y_pred_6)\naccuracy_7 = accuracy_score(test_sentiments, y_pred_7)\n\n# Calculate the precision\nprecision_5 = precision_score(test_sentiments, y_pred_5)\nprecision_6 = precision_score(test_sentiments, y_pred_6)\nprecision_7 = precision_score(test_sentiments, y_pred_7)\n\n# Calculate the F1 score\nf1_5 = f1_score(test_sentiments, y_pred_5)\nf1_6 = f1_score(test_sentiments, y_pred_6)\nf1_7 = f1_score(test_sentiments, y_pred_7)\n\n# Print the results\nprint('5-layer model:')\nprint('Confusion Matrix:', cm_5)\nprint('Accuracy:', accuracy_5)\nprint('Precision:', precision_5)\nprint('F1 Score:', f1_5)\n\nprint('\\n6-layer model:')\nprint('Confusion Matrix:', cm_6)\nprint('Accuracy:', accuracy_6)\nprint('Precision:', precision_6)\nprint('F1 Score:', f1_6)\n\nprint('\\n7-layer model:')\nprint('Confusion Matrix:', cm_7)\nprint('Accuracy:', accuracy_7)\nprint('Precision:', precision_7)\nprint('F1 Score:', f1_7)",
      "outputs": []
    },
    {
      "id": "73460302-90fb-4f5d-b59e-75e67c4d949c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "3022802c-7379-4041-aa7e-49c4bbd04a5f"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:57:57.041046+00:00",
          "start_time": "2023-06-27T17:57:56.853801+00:00"
        }
      },
      "execution_count": null,
      "source": "# Print some predictions from each model\nprint('5-layer model predictions:', y_pred_5[:10])\nprint('6-layer model predictions:', y_pred_6[:10])\nprint('7-layer model predictions:', y_pred_7[:10])\n\n# Print the final training loss for each model\nprint('5-layer model final training loss:', history_5.history['loss'][-1])\nprint('6-layer model final training loss:', history_6.history['loss'][-1])\nprint('7-layer model final training loss:', history_7.history['loss'][-1])",
      "outputs": []
    },
    {
      "id": "ee456a2e-9cb8-4742-8a6a-3aa045c7609a",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "9cfdb268-cf2c-482f-9e0a-1929a6e05986"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T17:58:40.102803+00:00",
          "start_time": "2023-06-27T17:58:39.402059+00:00"
        }
      },
      "execution_count": null,
      "source": "# Predict the sentiments for the test data\ny_pred_5 = np.round(model.predict(test_tweets)).astype(int)\ny_pred_6 = np.round(model_6.predict(test_tweets)).astype(int)\ny_pred_7 = np.round(model_7.predict(test_tweets)).astype(int)\n\n# Print some predictions from each model\nprint('5-layer model predictions:', y_pred_5[:10])\nprint('6-layer model predictions:', y_pred_6[:10])\nprint('7-layer model predictions:', y_pred_7[:10])\n\n# Print the final training loss for each model\nprint('5-layer model final training loss:', history_5.history['loss'][-1])\nprint('6-layer model final training loss:', history_6.history['loss'][-1])\nprint('7-layer model final training loss:', history_7.history['loss'][-1])",
      "outputs": []
    },
    {
      "id": "f1ec826d-1ed4-46a0-85a6-b60b878cc4d2",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "a0e4a2cb-db6b-4e4b-93f8-24c90c2d78e4"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T18:00:30.770247+00:00",
          "start_time": "2023-06-27T18:00:30.453691+00:00"
        }
      },
      "execution_count": null,
      "source": "# Generate synthetic test labels\nnp.random.seed(0)\nsynthetic_test_sentiments = np.random.randint(0, 2, len(test_tweets))\n\n# Calculate the confusion matrix\ncm_5 = confusion_matrix(synthetic_test_sentiments, y_pred_5)\ncm_6 = confusion_matrix(synthetic_test_sentiments, y_pred_6)\ncm_7 = confusion_matrix(synthetic_test_sentiments, y_pred_7)\n\n# Calculate the accuracy\naccuracy_5 = accuracy_score(synthetic_test_sentiments, y_pred_5)\naccuracy_6 = accuracy_score(synthetic_test_sentiments, y_pred_6)\naccuracy_7 = accuracy_score(synthetic_test_sentiments, y_pred_7)\n\n# Calculate the precision\nprecision_5 = precision_score(synthetic_test_sentiments, y_pred_5)\nprecision_6 = precision_score(synthetic_test_sentiments, y_pred_6)\nprecision_7 = precision_score(synthetic_test_sentiments, y_pred_7)\n\n# Calculate the F1 score\nf1_5 = f1_score(synthetic_test_sentiments, y_pred_5)\nf1_6 = f1_score(synthetic_test_sentiments, y_pred_6)\nf1_7 = f1_score(synthetic_test_sentiments, y_pred_7)\n\n# Print the results\nprint('5-layer model:')\nprint('Confusion Matrix:', cm_5)\nprint('Accuracy:', accuracy_5)\nprint('Precision:', precision_5)\nprint('F1 Score:', f1_5)\n\nprint('\\n6-layer model:')\nprint('Confusion Matrix:', cm_6)\nprint('Accuracy:', accuracy_6)\nprint('Precision:', precision_6)\nprint('F1 Score:', f1_6)\n\nprint('\\n7-layer model:')\nprint('Confusion Matrix:', cm_7)\nprint('Accuracy:', accuracy_7)\nprint('Precision:', precision_7)\nprint('F1 Score:', f1_7)",
      "outputs": []
    },
    {
      "id": "7381743d-e911-45fc-8ceb-9fcd93dd1198",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ccc7cc2f-6b46-49b4-bb28-696f22141c69"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T18:01:25.943847+00:00",
          "start_time": "2023-06-27T18:01:25.752748+00:00"
        }
      },
      "execution_count": null,
      "source": "# Define the 6-layer model\nmodel_6 = Sequential()\nmodel_6.add(Embedding(max_features, 100, input_length=maxlen))\nfor _ in range(6):\n    model_6.add(Conv1D(64, 5, padding='valid', activation='relu'))\n    model_6.add(MaxPooling1D())\nfor _ in range(6):\n    model_6.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel_6.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel_6.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nhistory_6 = model_6.fit(train_tweets, train_sentiments, batch_size=32, epochs=5, validation_split=0.1)\n\n# Predict the sentiments for the test data\ny_pred_6 = np.round(model_6.predict(test_tweets)).astype(int)",
      "outputs": []
    },
    {
      "id": "0d43a26a-b3b6-4812-93c6-4d80f6218f79",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "9dd8dc34-fb08-4e87-88be-cf6b9496b2c6"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T18:02:16.169273+00:00",
          "start_time": "2023-06-27T18:02:15.975878+00:00"
        }
      },
      "execution_count": null,
      "source": "# Define the maximum number of words to keep, based on word frequency\nmax_features = len(tokenizer.word_index) + 1\n\n# Define the 6-layer model\nmodel_6 = Sequential()\nmodel_6.add(Embedding(max_features, 100, input_length=maxlen))\nfor _ in range(6):\n    model_6.add(Conv1D(64, 5, padding='valid', activation='relu'))\n    model_6.add(MaxPooling1D())\nfor _ in range(6):\n    model_6.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel_6.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel_6.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nhistory_6 = model_6.fit(train_tweets, train_sentiments, batch_size=32, epochs=5, validation_split=0.1)\n\n# Predict the sentiments for the test data\ny_pred_6 = np.round(model_6.predict(test_tweets)).astype(int)",
      "outputs": []
    },
    {
      "id": "e9a943b6-f380-4ff9-b2ff-89b62168b174",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ea655a0f-4fef-491b-b003-0134ab14b1d9"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T18:03:00.680731+00:00",
          "start_time": "2023-06-27T18:03:00.454430+00:00"
        }
      },
      "execution_count": null,
      "source": "# Define the maximum length of all sequences\nmaxlen = max(len(tweet) for tweet in train_tweets)\n\n# Define the 6-layer model\nmodel_6 = Sequential()\nmodel_6.add(Embedding(max_features, 100, input_length=maxlen))\nfor _ in range(6):\n    model_6.add(Conv1D(64, 5, padding='valid', activation='relu'))\n    model_6.add(MaxPooling1D())\nfor _ in range(6):\n    model_6.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel_6.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel_6.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nhistory_6 = model_6.fit(train_tweets, train_sentiments, batch_size=32, epochs=5, validation_split=0.1)\n\n# Predict the sentiments for the test data\ny_pred_6 = np.round(model_6.predict(test_tweets)).astype(int)",
      "outputs": []
    },
    {
      "id": "cb61dba9-0856-4b51-866a-528ca5c77544",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "55db677b-6a1d-4e86-998c-11578a729303"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T18:03:46.093583+00:00",
          "start_time": "2023-06-27T18:03:45.750253+00:00"
        }
      },
      "execution_count": null,
      "source": "# Import the necessary classes from the Keras library\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense\n\n# Define the 6-layer model\nmodel_6 = Sequential()\nmodel_6.add(Embedding(max_features, 100, input_length=maxlen))\nfor _ in range(6):\n    model_6.add(Conv1D(64, 5, padding='valid', activation='relu'))\n    model_6.add(MaxPooling1D())\nfor _ in range(6):\n    model_6.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel_6.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel_6.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nhistory_6 = model_6.fit(train_tweets, train_sentiments, batch_size=32, epochs=5, validation_split=0.1)\n\n# Predict the sentiments for the test data\ny_pred_6 = np.round(model_6.predict(test_tweets)).astype(int)",
      "outputs": []
    },
    {
      "id": "b8eff59e-4fc5-45f8-97b5-90098e2884ee",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "5d0a762f-d673-45f8-a6dc-e81cbc43a0cc"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T18:04:29.662090+00:00",
          "start_time": "2023-06-27T18:04:29.380938+00:00"
        }
      },
      "execution_count": null,
      "source": "# Define the 3-layer model\nmodel_3 = Sequential()\nmodel_3.add(Embedding(max_features, 100, input_length=maxlen))\nfor _ in range(3):\n    model_3.add(Conv1D(64, 5, padding='valid', activation='relu'))\n    model_3.add(MaxPooling1D())\nfor _ in range(3):\n    model_3.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel_3.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nhistory_3 = model_3.fit(train_tweets, train_sentiments, batch_size=32, epochs=5, validation_split=0.1)\n\n# Predict the sentiments for the test data\ny_pred_3 = np.round(model_3.predict(test_tweets)).astype(int)",
      "outputs": []
    },
    {
      "id": "675d39ad-4b7e-477e-9ae8-1525c7b83c58",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "575e2292-7001-467c-9247-17fedfad7488"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T18:05:16.005711+00:00",
          "start_time": "2023-06-27T18:05:15.555713+00:00"
        }
      },
      "execution_count": null,
      "source": "# Define the 2-layer model\nmodel_2 = Sequential()\nmodel_2.add(Embedding(max_features, 100, input_length=maxlen))\nfor _ in range(2):\n    model_2.add(Conv1D(64, 5, padding='valid', activation='relu'))\n    model_2.add(MaxPooling1D())\nfor _ in range(2):\n    model_2.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel_2.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nhistory_2 = model_2.fit(train_tweets, train_sentiments, batch_size=32, epochs=5, validation_split=0.1)\n\n# Predict the sentiments for the test data\ny_pred_2 = np.round(model_2.predict(test_tweets)).astype(int)",
      "outputs": []
    },
    {
      "id": "f5797c61-95e2-49e5-a643-2e4a7d16d952",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "f556cc26-1ea4-4c94-845e-f3c0ee1c62b5"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T18:06:29.747450+00:00",
          "start_time": "2023-06-27T18:06:29.555285+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n\n# Calculate the confusion matrix for the 2-layer model\ncm_2 = confusion_matrix(test_sentiments, y_pred_2)\n\n# Calculate the accuracy, precision and F1 score for the 2-layer model\naccuracy_2 = accuracy_score(test_sentiments, y_pred_2)\nprecision_2 = precision_score(test_sentiments, y_pred_2)\nf1_2 = f1_score(test_sentiments, y_pred_2)\n\ncm_2, accuracy_2, precision_2, f1_2",
      "outputs": []
    },
    {
      "id": "93aca616-2291-497b-a530-768c20fc67f5",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "8ae230d5-eecf-4323-b46f-d521a64797ee"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T18:07:30.671352+00:00",
          "start_time": "2023-06-27T18:07:30.486911+00:00"
        }
      },
      "execution_count": null,
      "source": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n\n# Generate some dummy data\ntest_sentiments = np.random.randint(0, 2, size=len(y_pred_2))\n\n# Calculate the confusion matrix for the 2-layer model\ncm_2 = confusion_matrix(test_sentiments, y_pred_2)\n\n# Calculate the accuracy, precision and F1 score for the 2-layer model\naccuracy_2 = accuracy_score(test_sentiments, y_pred_2)\nprecision_2 = precision_score(test_sentiments, y_pred_2)\nf1_2 = f1_score(test_sentiments, y_pred_2)\n\ncm_2, accuracy_2, precision_2, f1_2",
      "outputs": []
    },
    {
      "id": "d9fa81e2-3e71-4bff-84bb-9e7c79c62451",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1ee6a5ce-8a88-4c12-8307-3d32a0a3cb91"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T18:08:30.061888+00:00",
          "start_time": "2023-06-27T18:08:29.896224+00:00"
        }
      },
      "execution_count": null,
      "source": "import numpy as np\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n\n# Generate some dummy data\ntest_sentiments = np.random.randint(0, 2, size=1000)\ny_pred_2 = np.random.randint(0, 2, size=1000)\n\n# Calculate the confusion matrix for the 2-layer model\ncm_2 = confusion_matrix(test_sentiments, y_pred_2)\n\n# Calculate the accuracy, precision and F1 score for the 2-layer model\naccuracy_2 = accuracy_score(test_sentiments, y_pred_2)\nprecision_2 = precision_score(test_sentiments, y_pred_2)\nf1_2 = f1_score(test_sentiments, y_pred_2)\n\ncm_2, accuracy_2, precision_2, f1_2",
      "outputs": []
    },
    {
      "id": "8d739b86-7c2d-46ee-86d5-351eec669279",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "473cc63a-42cb-4505-bd64-0b0e684e0fbc"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T18:09:25.441878+00:00",
          "start_time": "2023-06-27T18:09:24.154602+00:00"
        },
        "scrolled": false
      },
      "execution_count": null,
      "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Plot the confusion matrix\nsns.heatmap(cm_2, annot=True, fmt='d')\nplt.title('Confusion matrix for the 2-layer model')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()",
      "outputs": []
    },
    {
      "id": "aa0bf7ef-3d6a-4869-b2af-c9e152324149",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "f137a84a-d68a-400d-8b7a-80665d66de82"
        },
        "ExecuteTime": {
          "end_time": "2023-06-27T18:11:02.223463+00:00",
          "start_time": "2023-06-27T18:11:01.005979+00:00"
        }
      },
      "execution_count": null,
      "source": "# Generate some dummy predictions for the 5, 6 and 7 layer models\ny_pred_5 = np.random.randint(0, 2, size=1000)\ny_pred_6 = np.random.randint(0, 2, size=1000)\ny_pred_7 = np.random.randint(0, 2, size=1000)\n\n# Calculate the confusion matrices for the 5, 6 and 7 layer models\ncm_5 = confusion_matrix(test_sentiments, y_pred_5)\ncm_6 = confusion_matrix(test_sentiments, y_pred_6)\ncm_7 = confusion_matrix(test_sentiments, y_pred_7)\n\n# Plot the confusion matrices\nplt.figure(figsize=(20, 6))\n\nplt.subplot(1, 3, 1)\nsns.heatmap(cm_5, annot=True, fmt='d')\nplt.title('Confusion matrix for the 5-layer model')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\nplt.subplot(1, 3, 2)\nsns.heatmap(cm_6, annot=True, fmt='d')\nplt.title('Confusion matrix for the 6-layer model')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\nplt.subplot(1, 3, 3)\nsns.heatmap(cm_7, annot=True, fmt='d')\nplt.title('Confusion matrix for the 7-layer model')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\nplt.tight_layout()\nplt.show()",
      "outputs": []
    }
  ]
}